---
title: "Project and Data Organization"
author: "Michael Kistner"
date: "May 16, 2022"
output: 
  html_document:
    toc: true
---

# Introduction

Welcome! This notebook is the first in a five part series on data cleaning using the **tidyverse**. The intended audience is a quantitative social science practitioner who has previously used R, but who wants to learn more about data cleaning and is less familiar (or unfamiliar) with the collection of packages known as the **tidyverse**. Packages in the **tidyverse** all share a similar syntax and coding philosophy, and are designed to work together nicely.[^1]

[^1]: Individual packages we'll encounter include **dplyr**, **readr**, **tidyr**, **ggplot2**, **stringr**, and **lubridate**

The substantive focus of this short workshop is introducing a set of tools for data management, cleaning, manipulation, and exploration using R and the **tidyverse**. When thinking about the broader research process for a quantitative empirical project, an extremely simplified model of the process might be:

$$\textrm{Develop Question} \rightarrow \textrm{Collect Data} \rightarrow \underbrace{\textrm{Clean, Organize, and Transform Data}}_{\textrm{This workshop}} \rightarrow \textrm{Analyze Data} \rightarrow \textrm{Share Findings} $$

In a typical graduate program, students might cover steps 1 and 2 in a research design course, learn step 4 in their methods classes, and get practice at step 5 via seminar papers, writing a prospectus, and presenting research in a workshop. It's less typical for students to get formal training in step 3. While there is more material than we'll have time to cover in this workshop alone, we'll introduce a set of powerful tools for commonly encountered situations in computational social science. As we go, I'll share resources that will allow students to dive deeper into these topics. Much of the material I'm presenting is drawn from ["R for Data Science" by Hadley Wickham and Garrett Grolemund](https://r4ds.had.co.nz/index.html). In addition to showing you how to use tools, I'll also share my approach to project organization and management. This is a system developed via trial and error, and has proven incredibly useful to me, but there are alternative ways to organize and manage a project such as this.[^2] Less important than the specifics of the system is that *you have one*. Haphazardly throwing code, data, and results into a folder and hoping you can find what you need when you need it is a recipe for disaster, and the risk only scales as you work on more and bigger projects. Practices that might get you by in a seminar quickly become unworkable as a seminar paper turns into a prospectus, which turns into a dissertation, which (potentially!) turns into a research agenda.

[^2]: One useful guide that shares some, but not all, features in common with what I'll introduce here, is ["Code and Data for the Social Sciences: A Pracitioner's Guide" by Matthew Gentzkow and Jesse M. Shapiro](https://web.stanford.edu/~gentzkow/research/CodeAndData.xhtml).

# Refresher on RStudio and code documents

Before we go any further, let's take a moment to (re-)familiarize ourselves with what's going on when we open up RStudio. RStudio displays four different panes when you're working with code. The top left pane, where you're probably reading this, is known as the *Source* pane. If you're working on a notebook (like this document) or a script, it will appear here. You can also view datasets you're working with in this pane using RStudio's built-in data viewer. The bottom left pane is the *Console*, where you can run lines of code directly, although you can run code from a notebook or script as well. If you want to save code so everything you do is reproducible (which you should be doing 99% of the time), you need to make sure to put it in your script or notebook, as code you run in the console will not be saved. The top right pane shows (among other things) the *Environment*. When you load data or create objects, their names and details will appear in this pane, and the environment pane is useful for keeping track of what you've created, what variable names are, etc.. Finally, the bottom left pane shows the *File* directory. You can load data or open a notebook from here, for example. It's also where help files will appear if you ask R to explain how a function or command works, or plots be displayed if you're not working a notebook.

Speaking of which, this document is an *R Markdown Notebook*. The two most common types of code documents you'll work with are notebooks and scripts. An R script contains nothing but code and short comments about what the code is doing. It's meant primarily to accomplish one focused coding job (merging datasets, running a series of regressions, etc.) and do little else. In contrast, a notebook contains a combination of text, like what you're reading now, chunks of code that accomplish specific tasks, and plot output. You can add all sorts of formatting to make everything look neat, and you can compile the text, code, and output into a readable document like a PDF or HTML file using R's markdown language. I typically use a series of individual R scripts for data cleaning and management, but conduct analyses in a notebook that make it easy to go back and forth between sections and see previous output as I continue to analyze.

# Projects and directories

As a project progresses, the data, scripts, and notebooks collect. How do you 1) keep track of where everything is? and 2) make it easy for code to reference data and other code?

The two tools we'll discuss here are *RStudio projects* and *directory structure*. [RStudio projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects#:~:text=Using%20Projects,%2C%20history%2C%20and%20source%20documents) are a helpful way to keep individual projects separated, make it easy to move from one to the other, and free the user from having to worry about where on their computer individual files are.

About that latter point -- have you ever seen a line that looks something like:

```{r eval=FALSE}
setwd("C:\\Users\\NameAndOrNumbers\\OneDrive\\Documents\\R\\MyProject\\SpecificFolder")
```

or some similarly long and hyper-specific file location? This is bad practice for several reasons. First, it means that if you ever move any files on your computer anywhere else, your code won't run and you'll have to hunt down where the files live now. Second, it means that anyone who you share your code with will have to change things to match where files and folders are on their specific machine. All sorts of weird things can happen when finding and setting directories on different machines, when using different types of documents, etc.

RStudio projects avoid all of this by creating a *.Rproj* file that establishes a *root directory*. This is the directory in which everything associated with a project lives. When you open a project in RStudio, it automatically sets the directory to this root directory, allowing you to load data with a command as simple as:

```{r eval=FALSE}
load("Data/My Data.Rda")
```

Furthermore, anyone given access to the project and all of its folders can run that same command regardless of where they're running it or on what machine. Once you have multiple projects set up, you can cycle between them by clicking on the project name in the upper right-hand side of RStudio. Clicking on a project opens it, sets the directory to the root directory, and opens any previously open code documents so you can pick up where you left off. Setting up a project in RStudio is as easy as selecting "File" -\> "New Project" in the dropdown menu at the upper right. At this point, RStudio will ask if you want to create an entirely new directory for the project, or put the project in an existing directory.

This brings us to the next subject -- organizing the project directory. While it's possible with an RStudio project to have all files in the root directory, as projects get bigger and files accumulate this quickly becomes unwieldy. Instead, files should be organized into folders dedicated to specific types of file. For example, all data files in a folder titled "Data", all code scripts in a folder titled "Code", etc. For a specific research project, I typically create the following folders in my directory:

* Data
  + Contains all downloaded data files, unedited, as well as intermediate data files (i.e., after cleaning and manipulation). Some separate these into two folders.
* Code
  + Contains the scripts that take as an input all raw data to produce intermediate and final data files, as well as code to perform any analyses.
* Literature
  + Contains article and book summaries, annotated bibliographies, any downloaded papers I want to keep track of, etc.
* Figures and Tables
  + Output of analysis code that creates figures and tables that will be used in papers, presentations, or that I want to reference later.
* Papers and Presentations
  + Drafts of papers, presentation slides, and any other public-facing materials.

Depending on the specific project oftentimes other folders are necessary, but this is a good blueprint to follow. Furthermore, typically these folders will have subfolders as well. How intricate the subfolder structure is depends on your specific needs, and comes with tradeoffs. Adding additional subfolders might keep things cleaner within the folders and make it easier to find specific files, but come at the cost of increasing complexity and make referencing files in your code more cumbersome (i.e., "Folder/SubFolder/SubSubFolder/..."). Typically the simpler you can keep folder structure past the first level, the better.

# Version control

# Loading data
